{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQc-wXjqrEuR"
      },
      "source": [
        "# Quantization of Image Classification Models\n",
        "\n",
        "This tutorial demostrates how to apply INT8 quantization to Image Classification model using [Post-training Optimization Tool API](../../compression/api/README.md). The Mobilenet V2 model trained on the ImageNet-tiny dataset from Torchvision is used as an example. The code of this tutorial is designed to be extandable to custom model and dataset. It consists of the following steps:\n",
        "- Install OpenVINO and required tools and packages using PIP manager\n",
        "- Prepare the model for quantization\n",
        "- Prepare the ImageNet-tiny dataset\n",
        "- Define data loading and accuracy validation functionality\n",
        "- Run optimization pipeline\n",
        "- Compare accuracy of the original and quantized models\n",
        "- Compare performance of the original and quantized models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4cSNQWdbSyeo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from zipfile import ZipFile\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from addict import Dict\n",
        "\n",
        "sys.path.append('../utils')\n",
        "from notebook_utils import download_file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prepare dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Imagenet tiny with total 200 classes will be used. Note that mobilenet was trained on imagenet with 1000 classes and different preprocessing was used, so the results are expected to be lower for float model. \n",
        "\n",
        "Imagenet can be downloaded and checked too. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set the data and model directories\n",
        "DATA_DIR = 'data'\n",
        "MODEL_DIR = 'model'\n",
        "\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "download_path_data = 'http://cs231n.stanford.edu/tiny-imagenet-200.zip'\n",
        "data_name = 'tiny-imagenet-200.zip'\n",
        "\n",
        "download_file(download_path_data, directory=DATA_DIR, show_progress=True)\n",
        "with ZipFile(f'{DATA_DIR}/{data_name}', 'r') as zip_ref:\n",
        "    zip_ref.extractall(DATA_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-frbVLKrkmv"
      },
      "source": [
        "## Prepare the Model\n",
        "Model preparation stage has the following steps:\n",
        "- Download PyTorch model from Torchvision repository\n",
        "- Convert it to ONNX format\n",
        "- Run OpenVINO Model Optimizer tool to convert ONNX to OpenVINO Intermediate Representation (IR)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7i6dWUmhloy"
      },
      "outputs": [],
      "source": [
        "import torchvision.models as models\n",
        "\n",
        "# Export the model to ONNX format\n",
        "mobilenet_v2 = models.mobilenet_v2(pretrained=True)\n",
        "dummy_input = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "onnx_model_path = Path(MODEL_DIR) / 'mobilenet.onnx'\n",
        "ir_model_xml = onnx_model_path.with_suffix('.xml')\n",
        "ir_model_bin = onnx_model_path.with_suffix('.bin')\n",
        "\n",
        "torch.onnx.export(mobilenet_v2, dummy_input, onnx_model_path, verbose=True)\n",
        "\n",
        "# Run OpenVINO Model Optimization tool to convert ONNX to OpenVINO IR\n",
        "!mo --framework=onnx --data_type=FP16 --mean_values=[123.675,116.28,103.53] --input_shape=[1,3,224,224] --scale_values=[58.624,57.12,57.375] --reverse_input_channels -m $onnx_model_path  --output_dir $MODEL_DIR"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynLvh8rNc2wv"
      },
      "source": [
        "## Define Data Loader\n",
        "At this step the `DataLoader` interface from POT API is implemented. OpenCV Python is used for data reading and preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DErQofk8tO6c"
      },
      "outputs": [],
      "source": [
        "from cv2 import imread, resize as cv2_resize\n",
        "from compression.api import Metric, DataLoader\n",
        "\n",
        "\n",
        "def resize(image, params):\n",
        "    shape = params['height'], params['width']\n",
        "    return cv2_resize(image, shape)\n",
        "\n",
        "\n",
        "def crop(image, params):\n",
        "\n",
        "    height, width = image.shape[:2]\n",
        "\n",
        "    dst_height = int(height * params['central_fraction'])\n",
        "    dst_width = int(width * params['central_fraction'])\n",
        "\n",
        "    if height < dst_height or width < dst_width:\n",
        "        resized = np.array([width, height])\n",
        "        if width < dst_width:\n",
        "            resized *= dst_width / width\n",
        "        if height < dst_height:\n",
        "            resized *= dst_height / height\n",
        "        image = cv2_resize(image, tuple(np.ceil(resized).astype(int)))\n",
        "\n",
        "    top_left_y = (height - dst_height) // 2\n",
        "    top_left_x = (width - dst_width) // 2\n",
        "    return image[top_left_y:top_left_y + dst_height, top_left_x:top_left_x + dst_width]\n",
        "\n",
        "\n",
        "PREPROC_FNS = {'resize': resize, 'crop': crop}\n",
        "\n",
        "\n",
        "# Custom DataLoader class implementation that is required for\n",
        "# the proper reading of Imagenet images and annotations.\n",
        "class ImageNetDataLoader(DataLoader):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        if not isinstance(config, Dict):\n",
        "            config = Dict(config)\n",
        "        super().__init__(config)\n",
        "        self._annotations, self._img_ids = self._read_img_ids_annotations(config)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._img_ids)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if index >= len(self):\n",
        "            raise IndexError\n",
        "\n",
        "        annotation = (index, self._annotations[self._img_ids[index]])\\\n",
        "            if self._annotations else (index, None)\n",
        "        return annotation, self._read_image(self._img_ids[index])\n",
        "\n",
        "    # Methods specific to the current implementation\n",
        "    @staticmethod\n",
        "    def _read_img_ids_annotations(dataset):\n",
        "        \"\"\" Parses annotation file or directory with images to collect image names and annotations.\n",
        "        :param dataset: dataset config\n",
        "        :returns dictionary with annotations\n",
        "                 list of image ids\n",
        "        \"\"\"\n",
        "        annotations, annotations_decode = {}, {}\n",
        "        img_ids = []\n",
        "        if dataset.annotation_file:\n",
        "            with open(dataset.annotation_file, encoding='utf-8') as f:\n",
        "                for line in f:\n",
        "                    line = line.split(\"\\t\")\n",
        "                    img_id, annotation = line[0], line[1]\n",
        "                    try:\n",
        "                        annotation = annotations_decode[annotation]\n",
        "                    except KeyError:\n",
        "                        annotations_decode[annotation] = len(annotations_decode)\n",
        "                        annotation = annotations_decode[annotation]\n",
        "                      \n",
        "                    annotations[img_id] = annotation + 1 if dataset.has_background else annotation\n",
        "                    img_ids.append(img_id)\n",
        "        else:\n",
        "            img_ids = sorted(os.listdir(dataset.data_source))\n",
        "\n",
        "        return annotations, img_ids\n",
        "\n",
        "    def _read_image(self, index):\n",
        "        \"\"\" Reads images from directory.\n",
        "        :param index: image index to read\n",
        "        :return ndarray representation of image batch\n",
        "        \"\"\"\n",
        "        image = imread(os.path.join(self.config.data_source, index))\n",
        "        image = self._preprocess(image)\n",
        "        return image.transpose(2, 0, 1)\n",
        "\n",
        "    def _preprocess(self, image):\n",
        "        \"\"\" Does preprocessing of an image according to the preprocessing config.\n",
        "        :param image: ndarray image\n",
        "        :return processed image\n",
        "        \"\"\"\n",
        "        for prep_params in self.config.preprocessing:\n",
        "            image = PREPROC_FNS[prep_params.type](image, prep_params)\n",
        "        return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re9-YhbBddh3"
      },
      "source": [
        "## Define Accuracy Metric Calculation\n",
        "At this step the `Metric` interface for accuracy Top-1 metric is implemented. It is used for validating accuracy of quantized model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GB8L492ztZEC"
      },
      "outputs": [],
      "source": [
        "# Custom implementation of classification accuracy metric.\n",
        "class Accuracy(Metric):\n",
        "\n",
        "    # Required methods\n",
        "    def __init__(self, top_k=1):\n",
        "        super().__init__()\n",
        "        self._top_k = top_k\n",
        "        self._name = 'accuracy@top{}'.format(self._top_k)\n",
        "        self._matches = []\n",
        "\n",
        "    @property\n",
        "    def value(self):\n",
        "        \"\"\" Returns accuracy metric value for the last model output. \"\"\"\n",
        "        return {self._name: self._matches[-1]}\n",
        "\n",
        "    @property\n",
        "    def avg_value(self):\n",
        "        \"\"\" Returns accuracy metric value for all model outputs. \"\"\"\n",
        "        return {self._name: np.ravel(self._matches).mean()}\n",
        "\n",
        "    def update(self, output, target):\n",
        "        \"\"\" Updates prediction matches.\n",
        "        :param output: model output\n",
        "        :param target: annotations\n",
        "        \"\"\"\n",
        "        if len(output) > 1:\n",
        "            raise Exception('The accuracy metric cannot be calculated '\n",
        "                            'for a model with multiple outputs')\n",
        "        if isinstance(target, dict):\n",
        "            target = list(target.values())\n",
        "        predictions = np.argsort(output[0], axis=1)[:, -self._top_k:]\n",
        "        match = [float(t in predictions[i]) for i, t in enumerate(target)]\n",
        "\n",
        "        self._matches.append(match)\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\" Resets collected matches \"\"\"\n",
        "        self._matches = []\n",
        "\n",
        "    def get_attributes(self):\n",
        "        \"\"\"\n",
        "        Returns a dictionary of metric attributes {metric_name: {attribute_name: value}}.\n",
        "        Required attributes: 'direction': 'higher-better' or 'higher-worse'\n",
        "                             'type': metric type\n",
        "        \"\"\"\n",
        "        return {self._name: {'direction': 'higher-better',\n",
        "                             'type': 'accuracy'}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CclWk-fVd9Wi"
      },
      "source": [
        "## Run Quantization Pipeline and compare the accuracy of the original and quantized models\n",
        "Here we define a configuration for our quantization pipeline and run it. \n",
        "\n",
        "**Note**: we use built-in `IEEngine` implementation of the `Engine` interface from the POT API for model inference. `IEEngine` is built on top of OpenVINO Python* API for inference and provides basic functionality for inference of simple models, ImageNet pre-trained models. If you have a more complicated inference flow for your model/models you should create your own implementation of `Engine` interface, for example by inheriting from `IEEngine` and extending it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiAvrwo0tr6Z"
      },
      "outputs": [],
      "source": [
        "from compression.graph import load_model, save_model\n",
        "from compression.graph.model_utils import compress_model_weights\n",
        "from compression.engines.ie_engine import IEEngine\n",
        "from compression.pipeline.initializer import create_pipeline\n",
        "\n",
        "model_config = Dict({\n",
        "    'model_name': 'mobilenetv2',\n",
        "    'model': ir_model_xml,\n",
        "    'weights': ir_model_bin\n",
        "})\n",
        "engine_config = Dict({\n",
        "    'device': 'CPU',\n",
        "    'stat_requests_number': 2,\n",
        "    'eval_requests_number': 2\n",
        "})\n",
        "dataset_config = {\n",
        "    'data_source': os.path.join(DATA_DIR, 'tiny-imagenet-200/val/images'),\n",
        "    'annotation_file': os.path.join(DATA_DIR, 'tiny-imagenet-200/val/val_annotations.txt'),\n",
        "    'has_background': False,\n",
        "    'preprocessing': [\n",
        "        {\n",
        "            'type': 'crop',\n",
        "            'central_fraction': 0.875\n",
        "        },\n",
        "        {\n",
        "            'type': 'resize',\n",
        "            'width': 224,\n",
        "            'height': 224\n",
        "        }\n",
        "    ],\n",
        "}\n",
        "algorithms = [\n",
        "    {\n",
        "        'name': 'DefaultQuantization',\n",
        "        'params': {\n",
        "            'target_device': 'CPU',\n",
        "            'preset': 'performance',\n",
        "            'stat_subset_size': 300\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "# Steps 1-7: Model optimization\n",
        "# Step 1: Load the model.\n",
        "model = load_model(model_config)\n",
        "\n",
        "# Step 2: Initialize the data loader.\n",
        "data_loader = ImageNetDataLoader(dataset_config)\n",
        "\n",
        "# Step 3 (Optional. Required for AccuracyAwareQuantization): Initialize the metric.\n",
        "metric = Accuracy(top_k=1)\n",
        "\n",
        "# Step 4: Initialize the engine for metric calculation and statistics collection.\n",
        "engine = IEEngine(engine_config, data_loader, metric)\n",
        "\n",
        "# Step 5: Create a pipeline of compression algorithms.\n",
        "pipeline = create_pipeline(algorithms, engine)\n",
        "\n",
        "# Step 6: Execute the pipeline.\n",
        "compressed_model = pipeline.run(model)\n",
        "\n",
        "# Step 7 (Optional): Compress model weights quantized precision\n",
        "#                    in order to reduce the size of final .bin file.\n",
        "compress_model_weights(compressed_model)\n",
        "\n",
        "# Step 8: Save the compressed model to the desired path.\n",
        "compressed_model_paths = save_model(model=compressed_model, save_path=MODEL_DIR, model_name=\"quantized_mobilenet\"\n",
        ")\n",
        "compressed_model_xml = compressed_model_paths[0][\"model\"]\n",
        "\n",
        "# Step 9: Compare accuracy of the original and quantized models.\n",
        "metric_results = pipeline.evaluate(model)\n",
        "if metric_results:\n",
        "    for name, value in metric_results.items():\n",
        "        print('Accuracy of the original model: {: <27s}: {}'.format(name, value))\n",
        "\n",
        "metric_results = pipeline.evaluate(compressed_model)\n",
        "if metric_results:\n",
        "    for name, value in metric_results.items():\n",
        "        print('Accuracy of the optimized model: {: <27s}: {}'.format(name, value))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQACMfAUo52V"
      },
      "source": [
        "## Compare Performance of the Original and Quantized Models\n",
        "\n",
        "Finally, we will measure the inference performance of the FP32 and INT8 models. To do this, we use [Benchmark Tool](https://docs.openvinotoolkit.org/latest/openvino_inference_engine_tools_benchmark_tool_README.html) - OpenVINO's inference performance measurement tool.\n",
        "\n",
        "NOTE: For more accurate performance, we recommended running benchmark_app in a terminal/command prompt after closing other applications. Run benchmark_app -m model.xml -d CPU to benchmark async inference on CPU for one minute. Change CPU to GPU to benchmark on GPU. Run benchmark_app --help to see an overview of all command line options.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pC0gnO0c9-tI"
      },
      "outputs": [],
      "source": [
        "# Inference FP16 model (IR)\n",
        "!benchmark_app -m $ir_model_xml -d CPU -api async"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VR3-joFu9hH"
      },
      "outputs": [],
      "source": [
        "# Inference INT8 model (IR)\n",
        "!benchmark_app -m $compressed_model_xml -d CPU -api async"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "tutorial_tiny.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
